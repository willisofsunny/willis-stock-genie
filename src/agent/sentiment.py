import re
from typing import Any, List, Optional, Dict

from pydantic import Field

from src.agent.mcp import MCPAgent
from src.prompt.mcp import NEXT_STEP_PROMPT_ZN
from src.prompt.sentiment import SENTIMENT_SYSTEM_PROMPT
from src.schema import Message
from src.tool import Terminate, ToolCollection
from src.tool.sentiment import SentimentTool
from src.tool.web_search import WebSearch

import logging
logger = logging.getLogger(__name__)


class SentimentAgent(MCPAgent):
    """Sentiment analysis agent focused on market sentiment and news."""

    name: str = "sentiment_agent"
    description: str = "Analyzes market sentiment, news, and social media for insights on stock performance."
    system_prompt: str = SENTIMENT_SYSTEM_PROMPT
    next_step_prompt: str = NEXT_STEP_PROMPT_ZN

    # Initialize with FinGenius tools with proper type annotation
    available_tools: ToolCollection = Field(
        default_factory=lambda: ToolCollection(
            SentimentTool(),
            WebSearch(),
            Terminate(),
        )
    )

    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])

    async def run(
        self, request: Optional[str] = None, stock_code: Optional[str] = None
    ) -> Any:
        """Run sentiment analysis on the given stock.

        Args:
            request: Optional initial request to process. If provided, overrides stock_code parameter.
            stock_code: The stock code/ticker to analyze

        Returns:
            Dictionary containing sentiment analysis results
        """
        # If stock_code is provided but request is not, create request from stock_code
        if stock_code and not request:
            # Set up system message about the stock being analyzed
            self.memory.add_message(
                Message.system_message(
                    f"ä½ æ­£åœ¨åˆ†æžè‚¡ç¥¨ {stock_code} çš„å¸‚åœºæƒ…ç»ªã€‚è¯·æ”¶é›†ç›¸å…³æ–°é—»ã€ç¤¾äº¤åª’ä½“æ•°æ®ï¼Œå¹¶è¯„ä¼°æ•´ä½“æƒ…ç»ªã€‚"
                )
            )
            request = f"è¯·åˆ†æž {stock_code} çš„å¸‚åœºæƒ…ç»ªå’Œç›¸å…³æ–°é—»ã€‚"

        # Call parent implementation with the request
        result = await super().run(request)

        # Force generate summary if no analysis found in result
        if result and isinstance(result, str):
            # Check if result contains actual analysis or just tool outputs
            # Look for Chinese markdown headers (## èˆ†æƒ…, ## åˆ†æž, etc.) which indicate AI analysis
            has_chinese_analysis = bool(re.search(r'##\s*[\u4e00-\u9fa5]', result))
            if "Observed output of cmd" in result and not has_chinese_analysis:
                logger.info(f"âš ï¸ {self.name} finished without generating analysis, forcing summary generation...")

                # Add a forced prompt to generate summary
                summary_prompt = """
                åŸºæ–¼ä½ å‰›æ‰ç²å–çš„è¼¿æƒ…å’Œæ–°èžæ•¸æ“šï¼Œè«‹ç«‹å³æä¾›å®Œæ•´çš„è¼¿æƒ…åˆ†æžå ±å‘Šï¼ŒåŒ…æ‹¬ï¼š
                1. æ–°èžè¼¿æƒ…æ¦‚æ³ï¼ˆæœ€æ–°æ–°èžç†±é»žç¸½çµï¼‰
                2. å¸‚å ´æƒ…ç·’åˆ†æžï¼ˆæ•´é«”æƒ…ç·’å‚¾å‘ã€çœ‹å¤šçœ‹ç©ºæ¯”ä¾‹ï¼‰
                3. ç¤¾äº¤åª’é«”æƒ…ç·’ï¼ˆæŠ•è³‡è€…è¨Žè«–ç†±åº¦ã€ä¸»è¦è§€é»žï¼‰
                4. è¼¿æƒ…é¢¨éšªè©•ä¼°ï¼ˆè² é¢æ–°èžå½±éŸ¿ã€å¸‚å ´æƒ…ç·’é¢¨éšªï¼‰
                5. è¼¿æƒ…è¶¨å‹¢åˆ¤æ–·ï¼ˆçŸ­æœŸæƒ…ç·’èµ°å‘ã€ä¸­æœŸæƒ…ç·’é æœŸï¼‰
                6. æ“ä½œå»ºè­°ï¼ˆåŸºæ–¼è¼¿æƒ…çš„äº¤æ˜“å»ºè­°èˆ‡æ³¨æ„äº‹é …ï¼‰

                è«‹ç›´æŽ¥è¼¸å‡ºåˆ†æžå…§å®¹ï¼Œä¸è¦å†èª¿ç”¨å·¥å…·ã€‚
                """

                self.memory.add_message(Message.user_message(summary_prompt))

                # Generate summary without tools
                summary_response = await self.llm.ask(
                    messages=self.messages,
                    system_msgs=[Message.system_message(self.system_prompt)] if self.system_prompt else None
                )

                if summary_response:
                    logger.info(f"âœ… Generated forced summary for {self.name}")
                    result += f"\n\n{summary_response}"

        return result

    async def analyze(self, stock_code: str, **kwargs) -> Dict:
        """æ‰§è¡Œèˆ†æƒ…åˆ†æž"""
        try:
            logger.info(f"å¼€å§‹èˆ†æƒ…åˆ†æž: {stock_code}")
            
            # ç¡®ä¿å·¥å…·æ‰§è¡Œ - æ·»åŠ å¼ºåˆ¶æ‰§è¡Œé€»è¾‘
            analysis_tasks = []
            
            # 1. å¼ºåˆ¶æ‰§è¡Œæ–°é—»æœç´¢
            try:
                news_result = await self.tool_call("web_search", {
                    "query": f"{stock_code} è‚¡ç¥¨ æœ€æ–°æ¶ˆæ¯ èˆ†æƒ…",
                    "num_results": 10,
                    "lang": "zh",
                    "country": "tw"
                })
                if news_result and not news_result.error:
                    analysis_tasks.append(("news_search", news_result.model_dump()))
                    logger.info(f"æ–°é—»æœç´¢æˆåŠŸ: {stock_code}")
                else:
                    logger.warning(f"æ–°é—»æœç´¢å¤±è´¥: {stock_code}")
            except Exception as e:
                logger.error(f"æ–°é—»æœç´¢å¼‚å¸¸: {stock_code}, {str(e)}")
            
            # 2. å¼ºåˆ¶æ‰§è¡Œç¤¾äº¤åª’ä½“åˆ†æž
            try:
                social_result = await self.tool_call("web_search", {
                    "query": f"{stock_code} è¨Žè«–å€ æŠ•è³‡äºº æƒ…ç·’",
                    "num_results": 5,
                    "lang": "zh",
                    "country": "tw"
                })
                if social_result and not social_result.error:
                    analysis_tasks.append(("social_media", social_result.model_dump()))
                    logger.info(f"ç¤¾äº¤åª’ä½“åˆ†æžæˆåŠŸ: {stock_code}")
                else:
                    logger.warning(f"ç¤¾äº¤åª’ä½“åˆ†æžå¤±è´¥: {stock_code}")
            except Exception as e:
                logger.error(f"ç¤¾äº¤åª’ä½“åˆ†æžå¼‚å¸¸: {stock_code}, {str(e)}")
            
            # 3. å¼ºåˆ¶æ‰§è¡Œèˆ†æƒ…åˆ†æžå·¥å…·
            try:
                sentiment_result = await self.tool_call("sentiment_tool", {
                    "index_code": "000300",
                    "sector_types": "all",
                    "max_retry": 2
                })
                if sentiment_result and not sentiment_result.error:
                    analysis_tasks.append(("sentiment_analysis", sentiment_result.output))
                    logger.info(f"èˆ†æƒ…åˆ†æžå·¥å…·æˆåŠŸ: {stock_code}")
                else:
                    logger.warning(f"èˆ†æƒ…åˆ†æžå·¥å…·å¤±è´¥: {stock_code}")
            except Exception as e:
                logger.error(f"èˆ†æƒ…åˆ†æžå·¥å…·å¼‚å¸¸: {stock_code}, {str(e)}")
            
            # 4. ç»¼åˆåˆ†æžç»“æžœ
            if analysis_tasks:
                summary = self._generate_comprehensive_summary(analysis_tasks, stock_code)
                logger.info(f"èˆ†æƒ…åˆ†æžå®Œæˆ: {stock_code}, æ‰§è¡Œäº† {len(analysis_tasks)} ä¸ªä»»åŠ¡")
                return {
                    "success": True,
                    "analysis_count": len(analysis_tasks),
                    "summary": summary,
                    "tasks_executed": [task[0] for task in analysis_tasks]
                }
            else:
                logger.warning(f"èˆ†æƒ…åˆ†æžæ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•ä»»åŠ¡: {stock_code}")
                return {
                    "success": False,
                    "analysis_count": 0,
                    "summary": "æ— æ³•èŽ·å–èˆ†æƒ…æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥å’Œæ•°æ®æº",
                    "tasks_executed": []
                }
                
        except Exception as e:
            logger.error(f"èˆ†æƒ…åˆ†æžå¤±è´¥: {stock_code}, {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "analysis_count": 0,
                "summary": f"èˆ†æƒ…åˆ†æžå¼‚å¸¸: {str(e)}"
            }
    
    def _generate_comprehensive_summary(self, analysis_tasks: List, stock_code: str) -> str:
        """ç”Ÿæˆç»¼åˆèˆ†æƒ…åˆ†æžæŠ¥å‘Š"""
        try:
            summary_parts = [f"## {stock_code} èˆ†æƒ…åˆ†æžæŠ¥å‘Š\n"]
            
            for task_name, task_data in analysis_tasks:
                if task_name == "news_search":
                    summary_parts.append("### ðŸ“° æ–°é—»èˆ†æƒ…")
                    summary_parts.append(f"- æœç´¢åˆ° {len(task_data.get('results', []))} æ¡ç›¸å…³æ–°é—»")
                    summary_parts.append(f"- æ•´ä½“æƒ…ç»ªå€¾å‘: {self._analyze_news_sentiment(task_data)}")
                    
                elif task_name == "social_media":
                    summary_parts.append("### ðŸ’¬ ç¤¾äº¤åª’ä½“æƒ…ç»ª")
                    summary_parts.append(f"- æœç´¢åˆ° {len(task_data.get('results', []))} æ¡ç›¸å…³è®¨è®º")
                    summary_parts.append(f"- æŠ•èµ„è€…æƒ…ç»ª: {self._analyze_social_sentiment(task_data)}")
                    
                elif task_name == "sentiment_analysis":
                    summary_parts.append("### ðŸ“Š ä¸“ä¸šèˆ†æƒ…åˆ†æž")
                    summary_parts.append(f"- æƒ…ç»ªæŒ‡æ•°: {task_data.get('sentiment_score', 'N/A')}")
                    summary_parts.append(f"- é£Žé™©ç­‰çº§: {task_data.get('risk_level', 'N/A')}")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆèˆ†æƒ…åˆ†æžæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"èˆ†æƒ…åˆ†æžæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _analyze_news_sentiment(self, data: Dict) -> str:
        """åˆ†æžæ–°é—»æƒ…ç»ª"""
        try:
            results = data.get('results', [])
            if not results:
                return "ä¸­æ€§"
            
            # ç®€å•çš„å…³é”®è¯æƒ…ç»ªåˆ†æž
            positive_keywords = ["ä¸Šæ¶¨", "åˆ©å¥½", "çªç ´", "å¢žé•¿", "çœ‹å¥½"]
            negative_keywords = ["ä¸‹è·Œ", "åˆ©ç©º", "æš´è·Œ", "é£Žé™©", "äºæŸ"]
            
            positive_count = 0
            negative_count = 0
            
            for result in results:
                text = result.get('snippet', '') + result.get('title', '')
                for keyword in positive_keywords:
                    if keyword in text:
                        positive_count += 1
                for keyword in negative_keywords:
                    if keyword in text:
                        negative_count += 1
            
            if positive_count > negative_count:
                return "åæ­£é¢"
            elif negative_count > positive_count:
                return "åè´Ÿé¢"
            else:
                return "ä¸­æ€§"
        except:
            return "ä¸­æ€§"
    
    def _analyze_social_sentiment(self, data: Dict) -> str:
        """åˆ†æžç¤¾äº¤åª’ä½“æƒ…ç»ª"""
        try:
            results = data.get('results', [])
            if not results:
                return "å¹³æ·¡"
            
            # ç®€å•çš„è®¨è®ºçƒ­åº¦åˆ†æž
            discussion_keywords = ["ä¹°å…¥", "å–å‡º", "æŒæœ‰", "çœ‹æ¶¨", "çœ‹è·Œ"]
            keyword_count = 0
            
            for result in results:
                text = result.get('snippet', '') + result.get('title', '')
                for keyword in discussion_keywords:
                    if keyword in text:
                        keyword_count += 1
            
            if keyword_count >= 5:
                return "æ´»è·ƒ"
            elif keyword_count >= 2:
                return "ä¸€èˆ¬"
            else:
                return "å¹³æ·¡"
        except:
            return "å¹³æ·¡"
